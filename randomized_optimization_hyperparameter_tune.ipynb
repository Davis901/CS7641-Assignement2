{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "import sys\n",
    "import time\n",
    "import itertools\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "# import mlrose\n",
    "import joblib\n",
    "# sys.modules['sklearn.externals.joblib'] = j\n",
    "import mlrose_hiive as mlrose\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "weights= np.random.uniform(low=0.1, high=1, size=(100,))\n",
    "values= np.random.uniform(low=1, high=100, size=(100,))\n",
    "\n",
    "G = nx.Graph()\n",
    "    \n",
    "cluster_sizes = [10, 10, 10]\n",
    "base = 0\n",
    "for size in cluster_sizes:\n",
    "    cluster = nx.connected_watts_strogatz_graph(n=size, k=4, p=0.5, tries=100)\n",
    "    cluster = nx.relabel_nodes(cluster, {i: i + base for i in range(size)})\n",
    "    base += size\n",
    "    G = nx.compose(G, cluster)\n",
    "inter_cluster_edges = [(5, 15), (5, 25), (15, 20), (20, 25)]\n",
    "G.add_edges_from(inter_cluster_edges)\n",
    "edges = list(G.edges)\n",
    "\n",
    "\n",
    "fitness_functions = [('Four Peaks', mlrose.FourPeaks()), ('Max K-Color', mlrose.MaxKColor(edges=edges)),\\\n",
    "                     ('Continuous Peaks', mlrose.ContinuousPeaks()),('K-Napsak', mlrose.Knapsack(weights, values))]\n",
    "max_iterations = 5000\n",
    "max_attempts = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Hill Climbing - Restart tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searchg Parameter for:  Four Peaks\n",
      "Optimal Paratmeter for Four Peaks: 200\n",
      "\n",
      "Searchg Parameter for:  Max K-Color\n",
      "Optimal Paratmeter for Max K-Color: 100\n",
      "\n",
      "Searchg Parameter for:  Continuous Peaks\n",
      "Optimal Paratmeter for Continuous Peaks: 75\n",
      "\n",
      "Searchg Parameter for:  K-Napsak\n",
      "Optimal Paratmeter for K-Napsak: 25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_rhc_parameter = None\n",
    "best_rhc_fitness_value = None\n",
    "restart_candinate =[0, 25, 50, 75, 100, 125, 150, 175, 200]\n",
    "\n",
    "for fit_func in fitness_functions:\n",
    "    function_name = fit_func[0]\n",
    "    function = fit_func[1]\n",
    "\n",
    "    print(\"Searchg Parameter for: \", function_name)\n",
    "    best_rhc_parameter = None\n",
    "    best_rhc_fitness_value = None\n",
    "    \n",
    "    for r_val in restart_candinate:\n",
    "        problem_rhc = mlrose.DiscreteOpt(length=100, fitness_fn=function, maximize=True)\n",
    "        rhc_best_state, rhc_best_fitness, rhc_fitness_curve = mlrose.random_hill_climb(problem_rhc, \n",
    "                                                                                     max_attempts = max_attempts, \n",
    "                                                                                  max_iters=max_iterations, \n",
    "                                                                                     curve=True, \n",
    "                                                                                     random_state=42,\n",
    "                                                                                     restarts = r_val)\n",
    "\n",
    "        if not best_rhc_fitness_value:\n",
    "            best_rhc_parameter = r_val\n",
    "            best_rhc_fitness_value = rhc_best_fitness\n",
    "        elif rhc_best_fitness > best_rhc_fitness_value:\n",
    "            best_rhc_parameter = r_val\n",
    "            best_rhc_fitness_value = rhc_best_fitness\n",
    "\n",
    "    print(f\"Optimal Paratmeter for {function_name}: {best_rhc_parameter}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Annealing - init_temp, decay, min_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searchg Parameter for:  Four Peaks\n",
      "Optimal Paratmeter for Four Peaks: (4, 0.001), ExpDecay\n",
      "\n",
      "Searchg Parameter for:  Max K-Color\n",
      "Optimal Paratmeter for Max K-Color: (4, 0.001), ExpDecay\n",
      "\n",
      "Searchg Parameter for:  Continuous Peaks\n",
      "Optimal Paratmeter for Continuous Peaks: (4, 0.001), ExpDecay\n",
      "\n",
      "Searchg Parameter for:  K-Napsak\n",
      "Optimal Paratmeter for K-Napsak: (64, 0.001), ExpDecay\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_sa_parameter = None\n",
    "best_sa_fitness_value = None\n",
    "best_sa_decay = None\n",
    "sa_parameter_candidate = [\n",
    "    [1, 2, 4, 8, 16, 32, 64], #init_temp\n",
    "   [0.1, 0.2, 0.4, 0.8], #decay\n",
    "   [0.001, 0.01, 0.1, 1] #min_temp, exp_const\n",
    "]\n",
    "\n",
    "\n",
    "for fit_func in fitness_functions:\n",
    "    function_name = fit_func[0]\n",
    "    function = fit_func[1]\n",
    "\n",
    "    print(\"Searchg Parameter for: \", function_name)\n",
    "    best_sa_parameter = None\n",
    "    best_sa_fitness_value = None\n",
    "    best_sa_decay = None\n",
    "    \n",
    "    for sa_hyperparameter in itertools.product(*sa_parameter_candidate):\n",
    "        problem_sa = mlrose.DiscreteOpt(length=100, fitness_fn=function, maximize=True)\n",
    "        sa_decay_candidate = [(\"GeomDecay\",mlrose.GeomDecay(init_temp=sa_hyperparameter[0], decay=sa_hyperparameter[1], min_temp=sa_hyperparameter[2])),\\\n",
    "                 (\"ExpDecay\",mlrose.ExpDecay(init_temp=sa_hyperparameter[0], exp_const=sa_hyperparameter[2]))]\n",
    "\n",
    "        for d_val in sa_decay_candidate:                                                                                                                \n",
    "            sa_best_state, sa_best_fitness, sa_fitness_curve = mlrose.simulated_annealing(problem_sa, \n",
    "                                                                                        max_attempts = max_attempts, \n",
    "                                                                                        max_iters=max_iterations, \n",
    "                                                                                        curve=True, \n",
    "                                                                                        random_state=42,\n",
    "                                                                                        schedule = d_val[1])\n",
    "\n",
    "            if not best_sa_fitness_value:\n",
    "                best_sa_decay = d_val[0]\n",
    "                best_sa_parameter = sa_hyperparameter\n",
    "                best_sa_fitness_value = sa_best_fitness\n",
    "            elif sa_best_fitness > best_sa_fitness_value:\n",
    "                best_sa_decay = d_val[0]\n",
    "                best_sa_parameter = sa_hyperparameter\n",
    "                best_sa_fitness_value = sa_best_fitness\n",
    "\n",
    "    if best_sa_decay == \"ExpDecay\":\n",
    "        best_sa_parameter=(best_sa_parameter[0], best_sa_parameter[2])\n",
    "    print(f\"Optimal Paratmeter for {function_name}: {best_sa_parameter}, {best_sa_decay}\")\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Algorithm - pop_size, mutation_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searchg Parameter for:  Four Peaks\n",
      "Optimal Paratmeter for Four Peaks: (100, 0.8)\n",
      "\n",
      "Searchg Parameter for:  Max K-Color\n",
      "Optimal Paratmeter for Max K-Color: (100, 0.6)\n",
      "\n",
      "Searchg Parameter for:  Continuous Peaks\n",
      "Optimal Paratmeter for Continuous Peaks: (200, 0.4)\n",
      "\n",
      "Searchg Parameter for:  K-Napsak\n",
      "Optimal Paratmeter for K-Napsak: (600, 0.2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_ga_parameter = None\n",
    "best_ga_fitness_value = None\n",
    "ga_parameter_candidate = [\n",
    "    [100, 200, 400, 600], # pop_size\n",
    "   [0.2, 0.4, 0.5, 0.6,0.8] #mutation_prob\n",
    "]\n",
    "\n",
    "for fit_func in fitness_functions:\n",
    "    function_name = fit_func[0]\n",
    "    function = fit_func[1]\n",
    "\n",
    "    print(\"Searchg Parameter for: \", function_name)\n",
    "    best_ga_parameter = None\n",
    "    best_ga_fitness_value = None\n",
    "\n",
    "    for ga_hyperparameter in itertools.product(*ga_parameter_candidate):\n",
    "        problem_ga = mlrose.DiscreteOpt(length=100, fitness_fn=function, maximize=True)\n",
    "        ga_best_state, ga_best_fitness, ga_fitness_curve = mlrose.genetic_alg(\n",
    "                                                    problem_ga, \n",
    "                                                    max_attempts=max_attempts, \n",
    "                                                    max_iters=max_iterations, \n",
    "                                                    curve=True, \n",
    "                                                    random_state=42,\n",
    "                                                    pop_size=ga_hyperparameter[0],\n",
    "                                                    mutation_prob=ga_hyperparameter[1])\n",
    "        if not best_ga_fitness_value:\n",
    "            best_ga_parameter = ga_hyperparameter\n",
    "            best_ga_fitness_value = ga_best_fitness\n",
    "        elif ga_best_fitness > best_ga_fitness_value:\n",
    "            best_ga_parameter = ga_hyperparameter\n",
    "            best_ga_fitness_value = ga_best_fitness\n",
    "\n",
    "    print(f\"Optimal Paratmeter for {function_name}: {best_ga_parameter}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mimic - keep_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searchg Parameter for:  Four Peaks\n",
      "Optimal Paratmeter for Four Peaks: 0.1\n",
      "\n",
      "Searchg Parameter for:  Max K-Color\n",
      "Optimal Paratmeter for Max K-Color: 0.1\n",
      "\n",
      "Searchg Parameter for:  Continuous Peaks\n",
      "Optimal Paratmeter for Continuous Peaks: 0.25\n",
      "\n",
      "Searchg Parameter for:  K-Napsak\n",
      "Optimal Paratmeter for K-Napsak: 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_mm_parameter = None\n",
    "best_mm_fitness_value = None\n",
    "keep_pct_candinate =[0.1, 0.25, 0.4, 0.5, 0.75]\n",
    "\n",
    "for fit_func in fitness_functions:\n",
    "    function_name = fit_func[0]\n",
    "    function = fit_func[1]\n",
    "\n",
    "    print(\"Searchg Parameter for: \", function_name)\n",
    "    best_mm_parameter = None\n",
    "    best_mm_fitness_value = None\n",
    "    \n",
    "    for k_val in keep_pct_candinate:\n",
    "        problem_mm = mlrose.DiscreteOpt(length=100, fitness_fn=function, maximize=True)\n",
    "        mm_best_state, mm_best_fitness, mm_fitness_curve = mlrose.mimic(problem_mm, \n",
    "                                                                                     max_attempts=100, \n",
    "                                                                                     max_iters=100, \n",
    "                                                                                     curve=True, \n",
    "                                                                                     random_state=42,\n",
    "                                                                                     keep_pct=k_val)\n",
    "\n",
    "        if not best_mm_fitness_value:\n",
    "            best_mm_parameter = k_val\n",
    "            best_mm_fitness_value = mm_best_fitness\n",
    "        elif mm_best_fitness > best_mm_fitness_value:\n",
    "            best_mm_parameter = k_val\n",
    "            best_mm_fitness_value = mm_best_fitness\n",
    "\n",
    "    print(f\"Optimal Paratmeter for {function_name}: {best_mm_parameter}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
